{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd062b36ad15c9cb3cf8ba13ba90ae4a5eecbbb07a7ead0fedcbc156bc940ebce47",
   "display_name": "Python 3.8.8 64-bit ('torch': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import lib.medloaders as medical_loaders\n",
    "import lib.medzoo as medzoo\n",
    "# Lib files\n",
    "import lib.utils as utils\n",
    "from lib.utils.early_stopping import EarlyStopping\n",
    "from lib.utils.general import prepare_input\n",
    "from lib.losses3D import DiceLoss, create_loss,BCEDiceLoss\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from easydict import EasyDict\n",
    "\n",
    "from lib.metric3D.DiceCoefficient import DiceCoefficient\n",
    "from lib.metric3D.MeanIoU import MeanIoU\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = EasyDict({\n",
    "    \"batchSz\" : 1,\n",
    "    \"dataset_name\" : \"miccai2018\",\n",
    "    \"dim\" : (144,144,144),\n",
    "    \"nEpochs\" : 50,\n",
    "    \"classes\" : 1,\n",
    "    \"samples_train\" : 100,\n",
    "    \"samples_val\" : 10,\n",
    "    \"split\" : 0.8,\n",
    "    \"inChannels\" : 1,\n",
    "    \"inModalities\" : 1,\n",
    "    \"fold_id\" : '1',\n",
    "    \"lr\" : 1e-4,\n",
    "    \"cuda\" : True,\n",
    "    \"resume\" : '',\n",
    "    \"model\" : 'UNET3D', # VNET VECT2 UNET3D DENSENET1 DENSENET2 DENSENET3 HYPERDENSENET\n",
    "    \"opt\" : 'adam', # sgd adam rmsprop\n",
    "    \"log_dir\" : 'runs',\n",
    "    \"loadData\" : False,\n",
    "    \"terminal_show_freq\" : 10,\n",
    "    \"result_path\" : f'results/{time.strftime(\"%Y%m%d-%H%M%S\", time.localtime(time.time()))}'\n",
    "})\n",
    "\n",
    "shutil.rmtree(args.result_path, ignore_errors=True)\n",
    "os.mkdir(args.result_path)\n",
    "\n",
    "args.save = f'saved_models/{args.model}_checkpoints/{args.model}_{utils.datestr()}_{args.dataset_name}'\n",
    "args.save_checkpoint = os.path.join(args.save,'checkpoint.pt')\n",
    "args.tb_log_dir = f'runs/{args.model}_{utils.datestr()}_{args.dataset_name}'\n",
    "\n",
    "shutil.rmtree(args.tb_log_dir, ignore_errors=True)\n",
    "os.makedirs(args.tb_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "## FOR REPRODUCIBILITY OF RESULTS\n",
    "seed = 1777777\n",
    "utils.reproducibility(args, seed)\n",
    "\n",
    "utils.make_dirs(args.save)\n",
    "utils.save_arguments(args, args.save)"
   ]
  },
  {
   "source": [
    "## Tensorboard 설정"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# 기본 `log_dir` 은 \"runs\"이며, 여기서는 더 구체적으로 지정하였습니다\n",
    "writer = SummaryWriter(f'{args.tb_log_dir}/log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Building Model . . . . . . . .UNET3D\n",
      "UNET3D Number of params: 1781192\n",
      "Model transferred in GPU.....\n"
     ]
    }
   ],
   "source": [
    "# loss function\n",
    "# criterion = create_loss('CrossEntropyLoss')\n",
    "# criterion = BCEDiceLoss(classes=args.classes, weight=torch.tensor([0.1]).cuda())\n",
    "criterion = BCEDiceLoss(alpha=1, beta=1)\n",
    "\n",
    "# model & optimizer\n",
    "model, optimizer = medzoo.create_model(args)\n",
    "\n",
    "# model with cuda \n",
    "if args.cuda:\n",
    "    model = model.cuda()\n",
    "    print(\"Model transferred in GPU.....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, batch_size, train_generator, val_generator ,patience, n_epochs, metrics, tensorboard=None):\n",
    "\n",
    "    # 모델이 학습되는 동안 trainning loss를 track\n",
    "    train_losses = []\n",
    "    # 모델이 학습되는 동안 validation loss를 track\n",
    "    valid_losses = []\n",
    "    # epoch당 average training loss를 track\n",
    "    avg_train_losses = []\n",
    "    # epoch당 average validation loss를 track\n",
    "    avg_valid_losses = []\n",
    "\n",
    "    train_metrics = [[] for i in range(len(metrics))]\n",
    "    val_metrics = [[] for i in range(len(metrics))]\n",
    "\n",
    "    # early_stopping object의 초기화\n",
    "    early_stopping = EarlyStopping(patience = patience, verbose = True,path=args.save_checkpoint)\n",
    "\n",
    "    n_epochs_length = len(str(n_epochs))\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train() # prep model for training\n",
    "\n",
    "        progressbar = tqdm(train_generator)\n",
    "        for batch, input_tuple in enumerate(progressbar, start=1):\n",
    "        \n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()  \n",
    "\n",
    "            # gpu 연산으로 변경\n",
    "            input, target = prepare_input(input_tuple=input_tuple, args=args)\n",
    "            input.requires_grad = True\n",
    "\n",
    "            # forward pass: 입력된 값을 모델로 전달하여 예측 출력 계산\n",
    "            output = model(input)\n",
    "\n",
    "            # calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # backward pass: 모델의 파라미터와 관련된 loss의 그래디언트 계산\n",
    "            loss.backward()\n",
    "\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "\n",
    "            # record training loss\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            output[output > 0.5] = 1.0\n",
    "\n",
    "            # record matric\n",
    "            for i,metric in enumerate(metrics):\n",
    "                value = metric(output,target).item()\n",
    "                train_metrics[i].append(value)\n",
    "\n",
    "            # print metric & loss\n",
    "            print_msg = {\n",
    "                'loss' : f'{loss.item():.5f}',\n",
    "            }\n",
    "\n",
    "            for i,metric in enumerate(metrics):\n",
    "                print_msg[metric.metric_name] = f'{train_metrics[i][-1]:.5f}'\n",
    "\n",
    "            progressbar.set_postfix(print_msg)\n",
    "            progressbar.set_description(f'[{epoch:>{n_epochs_length}}/{n_epochs:>{n_epochs_length}}][{batch}/{len(train_generator)}]')\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval() # prep model for evaluation\n",
    "\n",
    "        prediction_images = []\n",
    "        for i,input_tuple in enumerate(val_generator) :\n",
    "\n",
    "            input, target = prepare_input(input_tuple=input_tuple, args=args)\n",
    "            input.requires_grad = True\n",
    "\n",
    "            # forward pass: 입력된 값을 모델로 전달하여 예측 출력 계산\n",
    "            output = model(input)\n",
    "\n",
    "            # calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # record validation loss\n",
    "            valid_losses.append(loss.item())\n",
    "\n",
    "            # save prediction result\n",
    "            if i < 10 :\n",
    "                prediction_images.append(\n",
    "                    [input.cpu().detach().numpy(),output.cpu().detach().numpy(), target.cpu().detach().numpy()])\n",
    "\n",
    "            output[output > 0.5] = 1.0\n",
    "\n",
    "            # metric\n",
    "            for i,metric in enumerate(metrics):\n",
    "                val_metrics[i].append(metric(output,target).item())\n",
    "\n",
    "        fig=plt.figure(figsize=(10, 10))\n",
    "        columns = 3\n",
    "        rows = 3\n",
    "\n",
    "        i = 1\n",
    "        for [input,output, target] in prediction_images:\n",
    "\n",
    "            fig.add_subplot(rows, columns, i); i+=1\n",
    "            plt.imshow(input[0,0,:,:,80])\n",
    "            plt.title(\"original Image\"); plt.axis('off')\n",
    "\n",
    "            fig.add_subplot(rows, columns, i); i+=1\n",
    "            plt.imshow(output[0,0,:,:, 80])\n",
    "            plt.title(\"Predicited Image\"); plt.axis('off')\n",
    "\n",
    "            fig.add_subplot(rows, columns, i); i+=1\n",
    "            plt.imshow(target[0,0,:,:, 80])\n",
    "            plt.title(\"Original Mask\"); plt.axis('off')\n",
    "\n",
    "        plt.savefig(f'{args.result_path}/{epoch}.png')\n",
    "\n",
    "\n",
    "        # print 학습/검증 statistics\n",
    "        # epoch당 평균 loss 계산\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "\n",
    "        # epoch당 평균 metric 계산\n",
    "        train_metric = {}\n",
    "        val_metric = {}\n",
    "\n",
    "        for i, metric in enumerate(metrics):\n",
    "            train_metric[metric.metric_name] = np.average(train_metrics[i])\n",
    "            val_metric[metric.metric_name] = np.average(val_metrics[i])\n",
    "\n",
    "\n",
    "        # epoch 결과 출력\n",
    "        print_msg = (f'[{epoch:>{n_epochs_length}}/{n_epochs:>{n_epochs_length}}] ' +\n",
    "                     f'loss: {train_loss:.5f} ' +\n",
    "                     f'val_loss: {valid_loss:.5f} ')\n",
    "\n",
    "        for key in train_metric.keys():\n",
    "            print_msg += f'{key} : {train_metric[key]:.5f} '\n",
    "\n",
    "        for key in val_metric.keys():\n",
    "            print_msg += f'val_{key} : {val_metric[key]:.5f} '\n",
    "\n",
    "        print(print_msg)\n",
    "\n",
    "\n",
    "        # tensorboard\n",
    "        if tensorboard:\n",
    "            writer.add_scalars(\"loss\", \n",
    "                                {\n",
    "                                    'train' : train_loss,\n",
    "                                    'val' : valid_loss\n",
    "                                 },\n",
    "                                epoch)\n",
    "\n",
    "            for train_key, val_key in zip(train_metric.keys(), val_metric.keys()):\n",
    "                writer.add_scalars(train_key,\n",
    "                                {\n",
    "                                    'train' : train_metric[train_key],\n",
    "                                     'val' : val_metric[val_key]\n",
    "                                }, \n",
    "                                 epoch)\n",
    "\n",
    "        # clear lists to track next epoch\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        train_metrics = [[] for i in range(len(metrics))]\n",
    "        val_metrics = [[] for i in range(len(metrics))]\n",
    "\n",
    "        # early_stopping는 validation loss가 감소하였는지 확인이 필요하며,\n",
    "        # 만약 감소하였을경우 현제 모델을 checkpoint로 만든다.\n",
    "        early_stopping(valid_loss, model)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "   # best model이 저장되어있는 last checkpoint를 로드한다.\n",
    "    model.load_state_dict(torch.load(args.save_checkpoint))\n",
    "    writer.close()\n",
    "\n",
    "    return  model, avg_train_losses, avg_valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "train : mri t2 flair and label .nii.gz to .npy: 100%|██████████| 386/386 [06:40<00:00,  1.04s/it]\n",
      "val : mri t2 flair and label .nii.gz to .npy: 100%|██████████| 98/98 [01:37<00:00,  1.00it/s]DATA SAMPLES HAVE BEEN GENERATED SUCCESSFULLY\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train generator & val generator\n",
    "training_generator, val_generator, full_volume, affine = medical_loaders.generate_datasets(args, path=r'datasets\\MICCAI_2018_Brain_Tumor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[ 1/50][264/386]:  68%|██████▊   | 264/386 [04:10<01:55,  1.05it/s, loss=0.55375, DiCE=0.86086, MeanIoU=0.74400]"
     ]
    }
   ],
   "source": [
    "meanIoU = MeanIoU(metric_name=\"MeanIoU\")\n",
    "diceCoefficient = DiceCoefficient(metric_name=\"DiCE\")\n",
    "\n",
    "model, train_loss, valid_loss = train_model(model = model,\n",
    "                                            train_generator = training_generator,\n",
    "                                            val_generator = val_generator,\n",
    "                                            batch_size = args.batchSz,\n",
    "                                            patience = 5,\n",
    "                                            n_epochs = args.nEpochs,\n",
    "                                            metrics=[diceCoefficient, meanIoU],\n",
    "                                            tensorboard=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 훈련이 진행되는 과정에 따라 loss를 시각화\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.plot(range(1,len(train_loss)+1),train_loss, label='Training Loss')\n",
    "plt.plot(range(1,len(valid_loss)+1),valid_loss,label='Validation Loss')\n",
    "\n",
    "# validation loss의 최저값 지점을 찾기\n",
    "minposs = valid_loss.index(min(valid_loss))+1\n",
    "plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
    "\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.ylim(0, 0.5) # 일정한 scale\n",
    "plt.xlim(0, len(train_loss)+1) # 일정한 scale\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig('loss_plot.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanIoU = MeanIoU(metric_name=\"MeanIoU\")\n",
    "diceCoefficient = DiceCoefficient(metric_name=\"DiCE\")\n",
    "\n",
    "model, train_loss, valid_loss = train_model(model = model,\n",
    "                                            train_generator = training_generator,\n",
    "                                            val_generator = val_generator,\n",
    "                                            batch_size = args.batchSz,\n",
    "                                            patience = 5,\n",
    "                                            n_epochs = args.nEpochs,\n",
    "                                            metrics=[diceCoefficient, meanIoU],\n",
    "                                            tensorboard=writer)"
   ]
  }
 ]
}