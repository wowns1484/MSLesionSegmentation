{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import shape\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF\n",
    "import numpy as np\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.BatchNorm3d(),\n",
    "            nn.Conv3d(in_channels, out_channels, 1, 1, 1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm3d(),\n",
    "            nn.Conv3d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ).to('cuda')\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "def ConvBlock(in_channels, out_channels):\n",
    "    block = nn.Sequential()\n",
    "    block.add_module(\n",
    "        nn.BatchNorm3d(),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv3d(in_channels, out_channels, 1),\n",
    "        nn.BatchNorm3d(),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv3d(in_channels, out_channels, 3)\n",
    "    )\n",
    "\n",
    "def TransitionBlock(in_channels, out_channels):\n",
    "    block = nn.Sequential()\n",
    "    block.add_module(\n",
    "        nn.BatchNorm3d(),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv3d(in_channels, out_channels, 1),\n",
    "        nn.AvgPool3d(2, 2)\n",
    "    )\n",
    "\n",
    "    return block\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, num_conva, in_channels, out_channels, **kwargs) -> None:\n",
    "        super(DenseBlock, self).__init__(**kwargs)\n",
    "        self.net = nn.Sequential()\n",
    "\n",
    "        for _ in range(num_conva):\n",
    "            self.net.add_module(ConvBlock(in_channels, out_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.net:\n",
    "            y = block(x)\n",
    "            x = torch.concat(x, y, dim=1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class UNET(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels=3, out_channels=1, features=[96, 128, 256, 512],\n",
    "    ):\n",
    "        super(UNET, self).__init__()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Down part of UNET\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        # Up part of UNET\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose3d(\n",
    "                    feature*2, feature, kernel_size=2, stride=2,\n",
    "                )\n",
    "            )\n",
    "            self.ups.append(DoubleConv(feature*2, feature))\n",
    "\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "        self.final_conv = nn.Conv3d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx//2]\n",
    "\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
    "\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx+1](concat_skip)\n",
    "\n",
    "        return self.final_conv(x)\n",
    "\n",
    "def test():\n",
    "    x = torch.randn((3, 1, 161, 161))\n",
    "    model = UNET(in_channels=1, out_channels=1)\n",
    "    preds = model(x)\n",
    "    assert preds.shape == x.shape\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "from easydict import EasyDict\n",
    "\n",
    "train_type = EasyDict({\n",
    "    \"TRAIN\" : 0,\n",
    "    \"VALIDATION\" : 1,\n",
    "    \"TEST\" : 2\n",
    "})\n",
    "\n",
    "data_type = EasyDict({\n",
    "    \"FLAIR\" : 0,\n",
    "    \"MPRAGE\" : 1,\n",
    "    \"PDW\" : 2,\n",
    "    \"T2W\" : 3,\n",
    "    \"MASK\" : 4\n",
    "})\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, growthRate = 12, dropRate = 0.0):\n",
    "        #input dimsnsion을 정하고, output dimension을 정하고(growh_rate임), dropRate를 정함.\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size = 7, stride = 2, padding = 0, bias = False)\n",
    "        self.bn = nn.BatchNorm3d(in_channels)\n",
    "        self.relu = nn.ReLU(inplace = True) # inplace 하면 input으로 들어온 것 자체를 수정하겠다는 뜻. 메모리 usage가 좀 좋아짐. 하지만 input을 없앰.\n",
    "        self.max_pool = nn.MaxPool3d(3, 2)\n",
    "        self.droprate = dropRate\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.max_pool(self.conv(self.relu(self.bn(x))))\n",
    "        if self.droprate > 0:\n",
    "            out = F.dropout (out, p = self.droprate, training = self.training)\n",
    "        \n",
    "        return out\n",
    "        # return torch.cat([x, out], dim=1)\n",
    "        \n",
    "class BottleneckBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropRate = 0.0):\n",
    "        #out_channels => growh_rate를 입력으로 받게 된다.\n",
    "        super(BottleneckBlock, self).__init__()\n",
    "        inter_planes = out_channels * 4 # bottleneck layer의 conv 1x1 filter chennel 수는 4*growh_rate이다.\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        self.conv1 = nn.Conv2d(in_channels, inter_planes, kernel_size=1,stride=1, padding=0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(inter_planes)\n",
    "        self.conv2 = nn.Conv2d(inter_planes, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.droprate = dropRate\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(self.relu(self.bn1(x)))\n",
    "        if self.droprate > 0:\n",
    "            out = F.dropout(out, p = self.droprate, inplace = False, training = self.training)\n",
    "        out = self.conv2(self.relu(self.bn2(out)))\n",
    "        if self.droprate > 0:\n",
    "            out = F.dropout(out, p = self.droprate, inplace = False, training = self.training)\n",
    "        return torch.cat([x, out], 1) # 입력으로 받은 x와 새로 만든 output을 합쳐서 내보낸다\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, nb_layers, in_channels, growh_rate, block,dropRate = 0.0):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.layer = self._make_layer(block, in_channels, growh_rate, nb_layers, dropRate)\n",
    "    \n",
    "    def _make_layer(self, block, in_channels, growh_rate, nb_layers, dropRate):\n",
    "        layers = []\n",
    "        for i in range(nb_layers):\n",
    "            layers.append(block(in_channels + i*growh_rate, growh_rate, dropRate))\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class TransitionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropRate=0.0):\n",
    "        super(TransitionBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = 1, stride = 1, padding = 0, bias = False)\n",
    "        self.droprate = dropRate\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(self.relu(self.bn1(x)))\n",
    "        if self.droprate > 0:\n",
    "            out = F.dropout(out, p=self.droprate, inplace = False, training = self.training)\n",
    "        return F.avg_pool2d(out, 2)\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, depth, num_classes, growh_rate = 12, reduction = 0.5, bottleneck = True, dropRate = 0.0):\n",
    "        super(DenseNet, self).__init__()\n",
    "        num_of_blocks = 3\n",
    "        in_channels = 16 # 2 * growh_rate\n",
    "        n = (depth - num_of_blocks - 1) / num_of_blocks # 총 depth에서 첫 conv , 2개의 transit , 마지막 linear 빼고 / num_of_blocks\n",
    "        if reduction != 1 :\n",
    "            in_channels = 2 * growh_rate\n",
    "        if bottleneck == True:\n",
    "            in_channels = 2 * growh_rate #논문에서 Bottleneck + Compression 할 경우 first layer은 2*growh_rate라고 했다.\n",
    "            n = n/2 # conv 1x1 레이어가 추가되니까 !\n",
    "            block = BottleneckBlock \n",
    "        else:\n",
    "            block = BasicBlock\n",
    "        \n",
    "        n = int(n) #n = DenseBlock에서 block layer 개수를 의미한다.\n",
    "        self.conv1 = nn.Conv2d(3, in_channels, kernel_size = 3, stride = 1, padding = 1, bias = False) # input:RGB -> output:growhR*2\n",
    "        \n",
    "        #1st block\n",
    "        # nb_layers,in_planes,growh_rate,block,dropRate\n",
    "        self.block1 = DenseBlock(n, in_channels, growh_rate, block, dropRate)\n",
    "        in_planes = int(in_planes + n*growh_rate) # 입력 + 레이어 만큼의 growh_rate\n",
    "        \n",
    "        # in_planes,out_planes,dropRate\n",
    "        self.trans1 = TransitionBlock(in_channels, int(math.floor(in_channels*reduction)), dropRate = dropRate)\n",
    "        in_planes = int(math.floor(in_channels*reduction))\n",
    "        \n",
    "        \n",
    "        #2nd block\n",
    "        # nb_layers,in_planes,growh_rate,block,dropRate\n",
    "        self.block2 = DenseBlock(n, in_channels, growh_rate, block,dropRate)\n",
    "        in_planes = int(in_channels + n*growh_rate) # 입력 + 레이어 만큼의 growh_rate\n",
    "        \n",
    "        # in_planes,out_planes,dropRate\n",
    "        self.trans2 = TransitionBlock(in_channels, int(math.floor(in_channels*reduction)), dropRate = dropRate)\n",
    "        in_planes = int(math.floor(in_channels*reduction))\n",
    "        \n",
    "        \n",
    "        #3rd block\n",
    "        # nb_layers,in_planes,growh_rate,block,dropRate\n",
    "        self.block3 = DenseBlock(n, in_channels, growh_rate, block, dropRate)\n",
    "        in_planes = int(in_channels + n*growh_rate) # 입력 + 레이어 만큼의 growh_rate\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        \n",
    "        self.fc = nn.Linear(in_channels, num_classes) # 마지막에 ave_pool 후에 1x1 size의 결과만 남음.\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "        # module 초기화\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # Conv layer들은 필터에서 나오는 분산 root(2/n)로 normalize 함\n",
    "                # mean = 0 , 분산 = sqrt(2/n) // 이게 무슨 초기화 방법이었는지 기억이 안난다.\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d): # shifting param이랑 scaling param 초기화(?)\n",
    "                m.weight.data.fill_(1) # \n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):# linear layer 초기화.\n",
    "                m.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x : 32*32\n",
    "        out = self.conv1(x) # 32*32\n",
    "        out = self.block1(out) # 32*32\n",
    "        out = self.trans1(out) # 16*16\n",
    "        out = self.block2(out) # 16*16\n",
    "        out = self.trans2(out) # 8*8\n",
    "        out = self.block3(out) # 8*8\n",
    "        out = self.relu(self.bn1(out)) #8*8\n",
    "        out = F.avg_pool2d(out, 8) #1*1\n",
    "        out = out.view(-1, self.in_planes) #channel수만 남기 때문에 Linear -> in_planes\n",
    "\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropRate = 0.0):\n",
    "        #input dimsnsion을 정하고, output dimension을 정하고(growh_rate임), dropRate를 정함.\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size = 7, stride = 2, padding = 3, bias = False)\n",
    "        self.bn = nn.BatchNorm3d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace = True) # inplace 하면 input으로 들어온 것 자체를 수정하겠다는 뜻. 메모리 usage가 좀 좋아짐. 하지만 input을 없앰.\n",
    "        self.max_pool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n",
    "        self.droprate = dropRate\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        if self.droprate > 0:\n",
    "            out = F.dropout (out, p = self.droprate, training = self.training)\n",
    "            \n",
    "        return out, self.max_pool(out)\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, num_conva, in_channels, growthRate = 48, **kwargs) -> None:\n",
    "        super(DenseBlock, self).__init__(**kwargs)\n",
    "        self.net = nn.Sequential()\n",
    "\n",
    "        for idx in range(num_conva):\n",
    "            self.net.add_module(name=f'Dense + {idx}', module=self.doubleConvBlock(in_channels + idx*growthRate, growthRate))\n",
    "    \n",
    "    def doubleConvBlock(self, in_channels, out_channels):\n",
    "        inter_planes = out_channels * 4\n",
    "        block = nn.Sequential(\n",
    "            nn.BatchNorm3d(in_channels),\n",
    "            nn.Conv3d(in_channels, inter_planes, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm3d(inter_planes),\n",
    "            nn.Conv3d(inter_planes, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        return block\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for block in self.net:\n",
    "            out = block(x)\n",
    "            x = torch.concat((x, out), dim=1)\n",
    "            print(x.shape)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class TransitionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropRate=0.0):\n",
    "        super(TransitionBlock, self).__init__()\n",
    "        self.bn = nn.BatchNorm3d(in_channels)\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.avg_pool = nn.AvgPool3d(kernel_size=2, stride=2)\n",
    "        self.droprate = dropRate\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv(self.relu(self.bn(x)))\n",
    "        if self.droprate > 0:\n",
    "            out = F.dropout(out, p=self.droprate, inplace=False, training=self.training)\n",
    "\n",
    "        out = self.avg_pool(out)\n",
    "        print(out.shape)\n",
    "\n",
    "        return out\n",
    "\n",
    "class UpSampleBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropRate=0.0):\n",
    "        super(UpSampleBlock, self).__init__()\n",
    "        self.bn = nn.BatchNorm3d(in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.transConv = nn.ConvTranspose3d(in_channels, out_channels, 3, stride=2, padding=0, bias=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "in_channels = [64, 128, 256, 512]\n",
    "num_blocks = [6, 12, 36, 24]\n",
    "grows_rate = 48\n",
    "\n",
    "x = torch.rand(1,1,208,224,208).cuda()\n",
    "basic = BasicBlock(1, in_channels[0]).cuda()\n",
    "dense = DenseBlock(num_blocks[0], in_channels[0], grows_rate).cuda()\n",
    "trans = TransitionBlock(in_channels[0] + grows_rate*num_blocks[0], in_channels[1]).cuda()\n",
    "test1 = basic(x)\n",
    "test2 = dense(test1[1])\n",
    "test3 = trans(test2)\n",
    "dense = DenseBlock(num_blocks[1], in_channels[1], grows_rate).cuda()\n",
    "trans = TransitionBlock(in_channels[1] + grows_rate*num_blocks[1], in_channels[2]).cuda()\n",
    "test4 = dense(test3)\n",
    "test5 = trans(test4)\n",
    "dense = DenseBlock(num_blocks[2], in_channels[2], grows_rate).cuda()\n",
    "trans = TransitionBlock(in_channels[2] + grows_rate*num_blocks[2], in_channels[3]).cuda()\n",
    "test6 = dense(test5)\n",
    "test7 = trans(test6)\n",
    "# dense = DenseBlock(num_blocks[3], in_channels[2], grows_rate)\n",
    "# trans = TransitionBlock(in_channels[2] + grows_rate*num_blocks[3], in_channels[3])\n",
    "# test8 = dense(test7)\n",
    "# test9 = trans(test8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1216, 6, 7, 6])\n",
      "torch.Size([1, 256, 13, 14, 13])\n",
      "torch.Size([1, 512, 13, 14, 13])\n",
      "torch.Size([1, 128, 26, 28, 26])\n",
      "torch.Size([1, 256, 26, 28, 26])\n",
      "torch.Size([1, 64, 52, 56, 52])\n",
      "torch.Size([1, 128, 52, 56, 52])\n",
      "torch.Size([1, 32, 104, 112, 104])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "running_mean should contain 154 elements not 160",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15548\\3992889491.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15548\\3992889491.py\u001b[0m in \u001b[0;36mtest\u001b[1;34m()\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m208\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m208\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cuda'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNestedDenseUNet3D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cuda'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m     \u001b[1;31m# assert preds.shape == x.shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15548\\3992889491.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1_3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1_3_up\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mx0_4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mups\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx0_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0_3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx1_3_up\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[0mouput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinal_convTrans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx0_4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15548\\3992889491.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDenseBlock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    177\u001b[0m             \u001b[0mbn_training\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[0mexponential_average_factor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m         )\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2282\u001b[0m     return torch.batch_norm(\n\u001b[1;32m-> 2283\u001b[1;33m         \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2284\u001b[0m     )\n\u001b[0;32m   2285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: running_mean should contain 154 elements not 160"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.BatchNorm3d(in_channels),\n",
    "            nn.Conv3d(in_channels, out_channels, 1, 1, 0, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.Conv3d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ).to('cuda')\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, num_conva, in_channels, growthRate = 48, **kwargs) -> None:\n",
    "        super(DenseBlock, self).__init__(**kwargs)\n",
    "        self.net = nn.Sequential()\n",
    "\n",
    "        for idx in range(num_conva):\n",
    "            self.net.add_module(name=f'Dense + {idx}', module=self.doubleConvBlock(in_channels + idx*growthRate, growthRate))\n",
    "    \n",
    "    def doubleConvBlock(self, in_channels, out_channels):\n",
    "        inter_planes = out_channels * 4\n",
    "        block = nn.Sequential(\n",
    "            nn.BatchNorm3d(in_channels),\n",
    "            nn.Conv3d(in_channels, inter_planes, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm3d(inter_planes),\n",
    "            nn.Conv3d(inter_planes, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        return block\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for block in self.net:\n",
    "            out = block(x)\n",
    "            x = torch.concat([x, out], dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "class TransitionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropRate=0.0):\n",
    "        super(TransitionBlock, self).__init__()\n",
    "        self.bn = nn.BatchNorm3d(in_channels)\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.avg_pool = nn.AvgPool3d(kernel_size=2, stride=2)\n",
    "        self.droprate = dropRate\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv(self.relu(self.bn(x)))\n",
    "        if self.droprate > 0:\n",
    "            out = F.dropout(out, p=self.droprate, inplace=False, training=self.training)\n",
    "\n",
    "        out = self.avg_pool(out)\n",
    "        return out\n",
    "\n",
    "in_channels_dense = [64, 128, 256, 512]\n",
    "num_blocks = [5, 10, 30, 20]\n",
    "grows_rate = 48\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropRate = 0.0):\n",
    "        #input dimsnsion을 정하고, output dimension을 정하고(growh_rate임), dropRate를 정함.\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size = 7, stride = 2, padding = 3, bias = False)\n",
    "        self.bn = nn.BatchNorm3d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace = True) # inplace 하면 input으로 들어온 것 자체를 수정하겠다는 뜻. 메모리 usage가 좀 좋아짐. 하지만 input을 없앰.\n",
    "        self.max_pool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n",
    "        self.droprate = dropRate\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        if self.droprate > 0:\n",
    "            out = F.dropout (out, p = self.droprate, training = self.training)\n",
    "        \n",
    "        return out, self.max_pool(out)\n",
    "\n",
    "class TransposeBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels) -> None:\n",
    "        super(TransposeBlock, self).__init__()\n",
    "        self.convTrans = nn.Sequential(\n",
    "            nn.BatchNorm3d(in_channels),\n",
    "            nn.Conv3d(in_channels, out_channels, 1, 1, 0, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ConvTranspose3d(out_channels, out_channels, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.convTrans(x)\n",
    "\n",
    "class NestedDenseUNet3D(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels=3, out_channels=1, features_encoder=[32, 64, 128, 256], features_decoder=[1216, 512, 256, 128], features_double=[1824, 800, 464, 154]\n",
    "    ):\n",
    "        super(NestedDenseUNet3D, self).__init__()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.downs_dense = nn.ModuleList()\n",
    "        self.downs_trans = nn.ModuleList()\n",
    "        self.basic_block = BasicBlock(1, 32)\n",
    "        self.pool = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.up = nn.Upsample(scale_factor=2)\n",
    "\n",
    "        # Down part of UNET\n",
    "        for idx, (feature, num_block) in enumerate(zip(features_encoder, num_blocks)):\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            self.downs_dense.append(DenseBlock(num_block, feature, grows_rate))\n",
    "            in_channels = feature\n",
    "\n",
    "            if idx < 3:\n",
    "                self.downs_trans.append(TransitionBlock(in_channels + num_block*grows_rate, features_encoder[idx+1]))\n",
    "\n",
    "        # Up part of UNET\n",
    "        for idx, (feature, double, encoder) in enumerate(zip(features_decoder, features_double, reversed(features_encoder))):\n",
    "            self.ups.append(TransposeBlock(feature, encoder))\n",
    "            self.ups.append(DoubleConv(double, encoder*2))\n",
    "\n",
    "        self.final_convTrans = nn.ConvTranspose3d(64, 32, kernel_size=2, stride=2)\n",
    "        self.final_conv = nn.Conv3d(32, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.basic_block(x)\n",
    "        x0_0 = x[0]\n",
    "        x1_0 = x[1]\n",
    "        x1_0_dense = self.downs_dense[0](x1_0)\n",
    "        x0_1 = DoubleConv(304, 30)\n",
    "        x0_1 = x0_1(torch.cat([x0_0, self.up(x1_0_dense)], 1))\n",
    "\n",
    "        x1_0 = self.downs_trans[0](x1_0_dense)\n",
    "\n",
    "        x2_0_dense = self.downs_dense[1](x1_0)\n",
    "        x1_1 = DoubleConv(816, 64)\n",
    "        x2_0_trans = TransposeBlock(x2_0_dense.shape[1], x2_0_dense.shape[1]).to('cuda')\n",
    "        x1_1 = x1_1(torch.cat([x1_0_dense, x2_0_trans(x2_0_dense)], 1))\n",
    "        x0_2 = DoubleConv(126, 30)\n",
    "        x1_1_trans = TransposeBlock(x1_1.shape[1], x1_1.shape[1]).to('cuda')\n",
    "        x0_2 = x0_2(torch.cat([x0_0, x0_1, x1_1_trans(x1_1)], 1))\n",
    "\n",
    "        x2_0 = self.downs_trans[1](x2_0_dense)\n",
    "\n",
    "        x3_0_dense = self.downs_dense[2](x2_0)\n",
    "        x2_1 = DoubleConv(2112, 128)\n",
    "        x3_0_trans = TransposeBlock(x3_0_dense.shape[1], x3_0_dense.shape[1]).to('cuda')\n",
    "        x2_1 = x2_1(torch.cat([x2_0_dense, x3_0_trans(x3_0_dense)], 1))\n",
    "        x1_2 = DoubleConv(464, 64)\n",
    "        x2_1_trans = TransposeBlock(x2_1.shape[1], x2_1.shape[1]).to('cuda')\n",
    "        x1_2 = x1_2(torch.cat([x1_0_dense, x1_1, x2_1_trans(x2_1)], 1))\n",
    "        x0_3 = DoubleConv(156, 30)\n",
    "        x1_2_trans = TransposeBlock(x1_2.shape[1], x1_2.shape[1]).to('cuda')\n",
    "        x0_3 = x0_3(torch.cat([x0_0, x0_1, x0_2, x1_2_trans(x1_2)], 1))\n",
    "\n",
    "        x3_0 = self.downs_trans[2](x3_0_dense)\n",
    "\n",
    "        x4_0 = self.downs_dense[3](x3_0)\n",
    "        print(x4_0.shape)\n",
    "        p3d = (0, 1, 0, 0, 0, 1)\n",
    "        x4_0_up = self.ups[0](x4_0)\n",
    "        x4_0_up = F.pad(x4_0_up, p3d)\n",
    "        print(x4_0_up.shape)\n",
    "        x3_1 = self.ups[1](torch.cat([x3_0_dense, x4_0_up], 1))\n",
    "        print(x3_1.shape)\n",
    "        x3_1_up = self.ups[2](x3_1)\n",
    "        print(x3_1_up.shape)\n",
    "        x2_2 = self.ups[3](torch.cat([x2_0_dense, x2_1, x3_1_up], 1))\n",
    "        x2_2_up = self.ups[4](x2_2)\n",
    "        print(x2_2.shape)\n",
    "        print(x2_2_up.shape)\n",
    "        x1_3 = self.ups[5](torch.cat([x1_0_dense, x1_1, x1_2, x2_2_up], 1))\n",
    "        x1_3_up = self.ups[6](x1_3)\n",
    "        print(x1_3.shape)\n",
    "        print(x1_3_up.shape)\n",
    "        x0_4 = self.ups[7](torch.cat([x0_0, x0_1, x0_2, x0_3, x1_3_up], 1))\n",
    "\n",
    "        ouput = self.final_convTrans(x0_4)\n",
    "        ouput = self.final_conv(ouput)\n",
    "        ouput = self.sigmoid(ouput)\n",
    "\n",
    "        return ouput\n",
    "\n",
    "def test():\n",
    "    x = torch.randn((1, 1, 208, 224, 208)).to('cuda')\n",
    "    model = NestedDenseUNet3D(in_channels=1, out_channels=1).to('cuda')\n",
    "    preds = model(x)\n",
    "    # assert preds.shape == x.shape\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.BatchNorm3d(in_channels),\n",
    "            nn.Conv3d(in_channels, out_channels, 1, 1, 0, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.Conv3d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        print(self.conv(x).shape)\n",
    "        return self.conv(x)\n",
    "\n",
    "def test():\n",
    "    x = torch.randn((1, 1, 208, 224, 208))\n",
    "    model = DoubleConv(in_channels=1, out_channels=32)\n",
    "    preds = model(x)\n",
    "    # assert preds.shape == x.shape\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af640ef5e237560663450db179292dda4e36d8af96316b6d53ad8d34500a15be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
