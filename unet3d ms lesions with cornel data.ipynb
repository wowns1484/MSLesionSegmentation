{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import lib.medloaders as medical_loaders\n",
    "import lib.medzoo as medzoo\n",
    "# Lib files\n",
    "import lib.utils as utils\n",
    "from lib.utils.early_stopping import EarlyStopping\n",
    "from lib.utils.general import prepare_input\n",
    "from lib.losses3D import DiceLoss, create_loss,BCEDiceLoss\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from easydict import EasyDict\n",
    "\n",
    "from lib.metric3D.DiceCoefficient import DiceCoefficient\n",
    "from lib.metric3D.MeanIoU import MeanIoU\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = EasyDict({\r\n",
    "    \"batch_size\" : 1,\r\n",
    "    \"test_name\" : \"MICCAI-2008-MSLESION-3DUnet-Flair\",\r\n",
    "    \"dataset_path\" : r\"datasets\\MICCAI_2008_MS_Lesions\", \r\n",
    "    \"dim\" : (144,144,144),\r\n",
    "    \"nEpochs\" : 100,\r\n",
    "    \"classes\" : 1,\r\n",
    "    \"split\" : (0.8, 0.1 , 0.1),\r\n",
    "    \"inChannels\" : 1,\r\n",
    "    \"inModalities\" : 1,\r\n",
    "    \"fold_id\" : '1',\r\n",
    "    \"lr\" : 1e-4,\r\n",
    "    \"cuda\" : True,\r\n",
    "    \"resume\" : '',\r\n",
    "    \"model\" : 'UNET3D', # VNET VECT2 UNET3D DENSENET1 DENSENET2 DENSENET3 HYPERDENSENET\r\n",
    "    \"opt\" : 'adam', # sgd adam rmsprop\r\n",
    "    \"log_dir\" : 'runs',\r\n",
    "    \"loadData\" : False,\r\n",
    "    \"terminal_show_freq\" : 10,\r\n",
    "})\r\n",
    "\r\n",
    "start_time = time.strftime(\"%Y%m%d-%H%M%S\", time.localtime(time.time()))\r\n",
    "\r\n",
    "args.result_path = rf'results/{args.test_name}-{start_time}'\r\n",
    "\r\n",
    "shutil.rmtree(args.result_path, ignore_errors=True)\r\n",
    "utils.make_dirs(args.result_path)\r\n",
    "\r\n",
    "args.save = rf'saved_models/{args.model}_checkpoints/{args.model}-{args.test_name}-{start_time}'\r\n",
    "args.save_checkpoint = os.path.join(args.save,'checkpoint.pt')\r\n",
    "args.tb_log_dir = rf'runs/{args.model}-{args.test_name}-{start_time}'\r\n",
    "\r\n",
    "utils.make_dirs(args.tb_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "## FOR REPRODUCIBILITY OF RESULTS\n",
    "seed = 1777777\n",
    "utils.reproducibility(args, seed)\n",
    "utils.make_dirs(args.save)\n",
    "utils.save_arguments(args, args.save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# 기본 `log_dir` 은 \"runs\"이며, 여기서는 더 구체적으로 지정하였습니다\n",
    "writer = SummaryWriter(f'{args.tb_log_dir}/log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchio as tio\r\n",
    "\r\n",
    "from torchio.transforms import (\r\n",
    "    RandomFlip,\r\n",
    "    RandomAffine,\r\n",
    "    RandomElasticDeformation, \r\n",
    "    RandomNoise,\r\n",
    "    RandomMotion,\r\n",
    "    RandomBiasField,\r\n",
    "    RescaleIntensity,\r\n",
    "    Resample,\r\n",
    "    ToCanonical,\r\n",
    "    ZNormalization,\r\n",
    "    CropOrPad,\r\n",
    "    HistogramStandardization,\r\n",
    "    OneOf,\r\n",
    "    Compose,\r\n",
    ")\r\n",
    "\r\n",
    "from lib.augment3D.random_crop_or_pad import RandomCropOrPad\r\n",
    "\r\n",
    "### Full-aug ###\r\n",
    "transform = tio.Compose([\r\n",
    "    RandomCropOrPad(args.dim),\r\n",
    "    RandomMotion(p=0.2),\r\n",
    "    RandomBiasField(p=0.3),\r\n",
    "    RandomNoise(p=0.5),\r\n",
    "    RandomFlip(axes=(0,)),\r\n",
    "    RandomAffine(p=0.5),\r\n",
    "    RandomElasticDeformation(p=0.5),\r\n",
    "    ZNormalization(),\r\n",
    "])\r\n",
    "\r\n",
    "### selectiv-aug ###\r\n",
    "# transform = tio.Compose([\r\n",
    "#     CropOrPad((144,144,144)),\r\n",
    "#     # RandomMotion(p=0.2),\r\n",
    "#     # RandomBiasField(p=0.3),\r\n",
    "#     RandomNoise(p=0.5),\r\n",
    "#     RandomFlip(axes=(0,)),\r\n",
    "#     # RandomAffine(p=0.5),\r\n",
    "#     RandomElasticDeformation(p=0.5),\r\n",
    "#     ZNormalization(),\r\n",
    "# ])\r\n",
    "\r\n",
    "validation_transform = tio.Compose([\r\n",
    "    CropOrPad(args.dim),\r\n",
    "    ZNormalization()\r\n",
    "])\r\n",
    "\r\n",
    "test_transform = tio.Compose([\r\n",
    "    CropOrPad(args.dim),\r\n",
    "    ZNormalization()\r\n",
    "])\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4075c8984ebf4fe3a4670147b7b71977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train : flair, mprage, t2w, pdw and label .nii.gz to .pt:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ae429c25d24755a610e6d0b6e6434e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lib.medloaders.miccai_2008_ms_lesions import MICCAI2008MSLESIONS\r\n",
    "from lib.medloaders.cornel_ms_lesions import CORNEL_MS_LESIONS\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "\r\n",
    "train_dataset = MICCAI2008MSLESIONS(train_mode='train', \r\n",
    "                                dataset_path=args.dataset_path,\r\n",
    "                                classes=args.classes,\r\n",
    "                                crop_dim=args.dim, \r\n",
    "                                split=args.split,\r\n",
    "                                transform=transform,\r\n",
    "                                sample_per_image=10)\r\n",
    "\r\n",
    "val_dataset = MICCAI2008MSLESIONS(train_mode='val',\r\n",
    "                                dataset_path=args.dataset_path,\r\n",
    "                                classes=args.classes, \r\n",
    "                                crop_dim=args.dim, \r\n",
    "                                split=args.split,\r\n",
    "                                transform=validation_transform,\r\n",
    "                                sample_per_image=1)\r\n",
    "\r\n",
    "\r\n",
    "test_dataset = MICCAI2008MSLESIONS(train_mode='test',\r\n",
    "                                dataset_path=args.dataset_path,\r\n",
    "                                classes=args.classes, \r\n",
    "                                crop_dim=args.dim, \r\n",
    "                                split=args.split,\r\n",
    "                                transform=test_transform,\r\n",
    "                                sample_per_image=1)\r\n",
    "\r\n",
    "params = {\r\n",
    "                'batch_size': args.batch_size,\r\n",
    "                'shuffle': False,\r\n",
    "                'num_workers': 4\r\n",
    "        }\r\n",
    "\r\n",
    "print(len(train_dataset))\r\n",
    "\r\n",
    "train_generator = DataLoader(train_dataset, **params)\r\n",
    "val_generator = DataLoader(val_dataset, **params)\r\n",
    "test_generator = DataLoader(test_dataset, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.medzoo.ResUnet3D import ResUNet3D\r\n",
    "from lib.medzoo.Unet3D import Unet3D\r\n",
    "import torch.optim as optim\r\n",
    "\r\n",
    "# model = ResUNet3D(in_channels=args.inChannels, n_classes=args.classes)\r\n",
    "model = Unet3D(in_channels=args.inChannels, out_channels=args.classes)\r\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-10)\r\n",
    "\r\n",
    "if torch.cuda.is_available():\r\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = BCEDiceLoss(alpha=1, beta=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_generator, val_generator ,patience, n_epochs, metrics, tensorboard=None):\r\n",
    "\r\n",
    "    # 모델이 학습되는 동안 trainning loss를 track\r\n",
    "    train_losses = []\r\n",
    "    # 모델이 학습되는 동안 validation loss를 track\r\n",
    "    valid_losses = []\r\n",
    "    # epoch당 average training loss를 track\r\n",
    "    avg_train_losses = []\r\n",
    "    # epoch당 average validation loss를 track\r\n",
    "    avg_valid_losses = []\r\n",
    "\r\n",
    "    train_metrics = [[] for i in range(len(metrics))]\r\n",
    "    val_metrics = [[] for i in range(len(metrics))]\r\n",
    "\r\n",
    "    # early_stopping object의 초기화\r\n",
    "    early_stopping = EarlyStopping(patience = patience, verbose = True,path=args.save_checkpoint)\r\n",
    "\r\n",
    "    n_epochs_length = len(str(n_epochs))\r\n",
    "    for epoch in range(1, n_epochs + 1):\r\n",
    "        ###################\r\n",
    "        # train the model #\r\n",
    "        ###################\r\n",
    "        model.train() # prep model for training\r\n",
    "\r\n",
    "        progressbar = tqdm(train_generator)\r\n",
    "        for batch, input_tuple in enumerate(progressbar, start=1):\r\n",
    "        \r\n",
    "            # clear the gradients of all optimized variables\r\n",
    "            optimizer.zero_grad()\r\n",
    "\r\n",
    "            # gpu 연산으로 변경\r\n",
    "            input, target = prepare_input(input_tuple=input_tuple, args=args)\r\n",
    "            input.requires_grad = True\r\n",
    "\r\n",
    "            # forward pass: 입력된 값을 모델로 전달하여 예측 출력 계산\r\n",
    "            output = model(input)\r\n",
    "\r\n",
    "            output = output.cpu()\r\n",
    "            target = target.cpu()\r\n",
    "\r\n",
    "            # calculate the loss\r\n",
    "            loss = criterion(output, target)\r\n",
    "\r\n",
    "            # backward pass: 모델의 파라미터와 관련된 loss의 그래디언트 계산\r\n",
    "            loss.backward()\r\n",
    "\r\n",
    "            # perform a single optimization step (parameter update)\r\n",
    "            optimizer.step()\r\n",
    "\r\n",
    "            # record training loss\r\n",
    "            train_losses.append(loss.item())\r\n",
    "\r\n",
    "            output[output > 0.5] = 1.0\r\n",
    "\r\n",
    "            # record matric\r\n",
    "            for i,metric in enumerate(metrics):\r\n",
    "                value = metric(output,target).item()\r\n",
    "                train_metrics[i].append(value)\r\n",
    "\r\n",
    "            # print metric & loss\r\n",
    "            print_msg = {\r\n",
    "                'loss' : f'{np.average(train_losses):.5f}',\r\n",
    "            }\r\n",
    "\r\n",
    "            for i,metric in enumerate(metrics):\r\n",
    "                print_msg[metric.metric_name] = f'{np.average(train_metrics[i]):.5f}'\r\n",
    "\r\n",
    "            progressbar.set_postfix(print_msg)\r\n",
    "            progressbar.set_description(f'[{epoch:>{n_epochs_length}}/{n_epochs:>{n_epochs_length}}][{batch}/{len(train_generator)}]')\r\n",
    "\r\n",
    "        ######################    \r\n",
    "        # validate the model #\r\n",
    "        ######################\r\n",
    "        model.eval() # prep model for evaluation\r\n",
    "\r\n",
    "        prediction_images = []\r\n",
    "\r\n",
    "        with torch.no_grad():\r\n",
    "            for i,input_tuple in enumerate(val_generator) :\r\n",
    "\r\n",
    "                input, target = prepare_input(input_tuple=input_tuple, args=args)\r\n",
    "                # input.requires_grad = True\r\n",
    "\r\n",
    "                # forward pass: 입력된 값을 모델로 전달하여 예측 출력 계산\r\n",
    "                output = model(input)\r\n",
    "\r\n",
    "                input = input.cpu()\r\n",
    "                output = output.cpu()\r\n",
    "                target = target.cpu()\r\n",
    "\r\n",
    "                # calculate the loss\r\n",
    "                loss = criterion(output, target)\r\n",
    "\r\n",
    "                # record validation loss\r\n",
    "                valid_losses.append(loss.item())\r\n",
    "\r\n",
    "                # save prediction result\r\n",
    "                prediction_images.append(\r\n",
    "                    [input.detach().numpy(),output.detach().numpy(), target.detach().numpy()])\r\n",
    "\r\n",
    "                output[output > 0.5] = 1.0\r\n",
    "\r\n",
    "                # metric\r\n",
    "                for i,metric in enumerate(metrics):\r\n",
    "                    val_metrics[i].append(metric(output,target).item())\r\n",
    "\r\n",
    "        columns = 3\r\n",
    "        rows = len(generator) * args.batch_size\r\n",
    "\r\n",
    "        fig=plt.figure(figsize=( 3 * columns,3 * rows))\r\n",
    "\r\n",
    "        i = 1\r\n",
    "        for [input,output, target] in prediction_images:\r\n",
    "\r\n",
    "            for batch_idx in args.batch_size:\r\n",
    "                fig.add_subplot(rows, columns, i); i+=1\r\n",
    "                plt.imshow(input[0,0,:,:,input.shape[-1] // 2], cmap='gray')\r\n",
    "                plt.title(\"original Image\"); plt.axis('off')\r\n",
    "\r\n",
    "                fig.add_subplot(rows, columns, i); i+=1\r\n",
    "                plt.imshow(output[0,0,:,:, input.shape[-1] // 2], cmap='gray')\r\n",
    "                plt.title(\"Predicited Image\"); plt.axis('off')\r\n",
    "\r\n",
    "                fig.add_subplot(rows, columns, i); i+=1\r\n",
    "                plt.imshow(target[0,0,:,:, input.shape[-1] // 2], cmap='gray')\r\n",
    "                plt.title(\"Original Mask\"); plt.axis('off')\r\n",
    "\r\n",
    "        plt.savefig(f'{args.result_path}/{epoch}.png')\r\n",
    "        plt.clf()\r\n",
    "\r\n",
    "        # print 학습/검증 statistics\r\n",
    "        # epoch당 평균 loss 계산\r\n",
    "        train_loss = np.average(train_losses)\r\n",
    "        valid_loss = np.average(valid_losses)\r\n",
    "        avg_train_losses.append(train_loss)\r\n",
    "        avg_valid_losses.append(valid_loss)\r\n",
    "\r\n",
    "        # epoch당 평균 metric 계산\r\n",
    "        train_metric = {}\r\n",
    "        val_metric = {}\r\n",
    "\r\n",
    "        for i, metric in enumerate(metrics):\r\n",
    "            train_metric[metric.metric_name] = np.average(train_metrics[i])\r\n",
    "            val_metric[metric.metric_name] = np.average(val_metrics[i])\r\n",
    "\r\n",
    "\r\n",
    "        # epoch 결과 출력\r\n",
    "        print_msg = (f'[{epoch:>{n_epochs_length}}/{n_epochs:>{n_epochs_length}}] ' +\r\n",
    "                     f'loss: {train_loss:.5f} ' +\r\n",
    "                     f'val_loss: {valid_loss:.5f} ')\r\n",
    "\r\n",
    "        for key in train_metric.keys():\r\n",
    "            print_msg += f'{key} : {train_metric[key]:.5f} '\r\n",
    "\r\n",
    "        for key in val_metric.keys():\r\n",
    "            print_msg += f'val_{key} : {val_metric[key]:.5f} '\r\n",
    "\r\n",
    "        # tensorboard\r\n",
    "        if tensorboard:\r\n",
    "            writer.add_scalars(\"loss\", \r\n",
    "                                {\r\n",
    "                                    'train' : train_loss,\r\n",
    "                                    'val' : valid_loss\r\n",
    "                                 },\r\n",
    "                                epoch)\r\n",
    "\r\n",
    "            for train_key, val_key in zip(train_metric.keys(), val_metric.keys()):\r\n",
    "                writer.add_scalars(train_key,\r\n",
    "                                {\r\n",
    "                                    'train' : train_metric[train_key],\r\n",
    "                                     'val' : val_metric[val_key]\r\n",
    "                                }, \r\n",
    "                                 epoch)\r\n",
    "\r\n",
    "        # clear lists to track next epoch\r\n",
    "        train_losses = []\r\n",
    "        valid_losses = []\r\n",
    "        train_metrics = [[] for i in range(len(metrics))]\r\n",
    "        val_metrics = [[] for i in range(len(metrics))]\r\n",
    "\r\n",
    "        # early_stopping는 validation loss가 감소하였는지 확인이 필요하며,\r\n",
    "        # 만약 감소하였을경우 현제 모델을 checkpoint로 만든다.\r\n",
    "        early_stopping(valid_loss, model)\r\n",
    "\r\n",
    "        if early_stopping.early_stop:\r\n",
    "            print(\"Early stopping\")\r\n",
    "            break\r\n",
    "\r\n",
    "   # best model이 저장되어있는 last checkpoint를 로드한다.\r\n",
    "    model.load_state_dict(torch.load(args.save_checkpoint))\r\n",
    "    writer.close()\r\n",
    "\r\n",
    "    return  model, avg_train_losses, avg_valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, generator, tensorboard, metrics, args):\r\n",
    "    model.eval() # prep model for evaluation\r\n",
    "\r\n",
    "    prediction_images = []\r\n",
    "\r\n",
    "    eval_losses = []\r\n",
    "    eval_metrics = [[] for i in range(len(metrics))]\r\n",
    "\r\n",
    "    with torch.no_grad():\r\n",
    "\r\n",
    "        progressbar = tqdm(generator)\r\n",
    "\r\n",
    "        for batch,input_tuple in enumerate(progressbar, start=1) :\r\n",
    "\r\n",
    "            input, target = prepare_input(input_tuple=input_tuple, args=args)\r\n",
    "            # input.requires_grad = True\r\n",
    "\r\n",
    "            # forward pass: 입력된 값을 모델로 전달하여 예측 출력 계산\r\n",
    "            output = model(input)\r\n",
    "\r\n",
    "            input = input.cpu()\r\n",
    "            output = output.cpu()\r\n",
    "            target = target.cpu()\r\n",
    "\r\n",
    "            # calculate the loss\r\n",
    "            loss = criterion(output, target)\r\n",
    "\r\n",
    "            # record validation loss\r\n",
    "            eval_losses.append(loss.item())\r\n",
    "\r\n",
    "            # save prediction result\r\n",
    "            prediction_images.append(\r\n",
    "                [input.detach().numpy(),output.detach().numpy(), target.detach().numpy()])\r\n",
    "\r\n",
    "            output[output > 0.5] = 1.0\r\n",
    "\r\n",
    "            # metric\r\n",
    "            for i,metric in enumerate(metrics):\r\n",
    "                eval_metrics[i].append(metric(output,target).item())\r\n",
    "\r\n",
    "    columns = 3\r\n",
    "    rows = len(generator) * args.batch_size\r\n",
    "\r\n",
    "    fig=plt.figure(figsize=( 3 * columns,3 * rows))\r\n",
    "\r\n",
    "    i = 1\r\n",
    "    for [input,output, target] in prediction_images:\r\n",
    "\r\n",
    "        for batch_idx in args.batch_size:\r\n",
    "            fig.add_subplot(rows, columns, i); i+=1\r\n",
    "            plt.imshow(input[batch_idx,0,:,:,input.shape[-1] // 2], cmap='gray')\r\n",
    "            plt.title(\"original Image\"); plt.axis('off')\r\n",
    "\r\n",
    "            fig.add_subplot(rows, columns, i); i+=1\r\n",
    "            plt.imshow(output[batch_idx,0,:,:, input.shape[-1] // 2], cmap='gray')\r\n",
    "            plt.title(\"Predicited Image\"); plt.axis('off')\r\n",
    "\r\n",
    "            fig.add_subplot(rows, columns, i); i+=1\r\n",
    "            plt.imshow(target[batch_idx,0,:,:, input.shape[-1] // 2], cmap='gray')\r\n",
    "            plt.title(\"Original Mask\"); plt.axis('off')\r\n",
    "\r\n",
    "    plt.savefig(f'{args.result_path}/test.png')\r\n",
    "    plt.clf()\r\n",
    "\r\n",
    "    # print 학습/검증 statistics\r\n",
    "    # epoch당 평균 loss 계산\r\n",
    "    eval_loss = np.average(eval_losses)\r\n",
    "\r\n",
    "    # epoch당 평균 metric 계산\r\n",
    "    eval_metric = {}\r\n",
    "\r\n",
    "    for i, metric in enumerate(metrics):\r\n",
    "        eval_metric[metric.metric_name] = np.average(eval_metrics[i])\r\n",
    "\r\n",
    "\r\n",
    "    # epoch 결과 출력\r\n",
    "    print_msg = (f'loss: {eval_loss:.5f} ')\r\n",
    "\r\n",
    "    for key in eval_metric.keys():\r\n",
    "        print_msg += f'{key} : {eval_metric[key]:.5f} '\r\n",
    "\r\n",
    "    # tensorboard\r\n",
    "    if tensorboard:\r\n",
    "        writer.add_scalars(\r\n",
    "                            \"loss\", \r\n",
    "                            {\r\n",
    "                                'eval' : eval_loss,\r\n",
    "                            },\r\n",
    "                            0)\r\n",
    "\r\n",
    "        for eval_key in eval_metric.keys():\r\n",
    "            writer.add_scalars(\r\n",
    "                                eval_key,\r\n",
    "                                {\r\n",
    "                                    'eval' : eval_metric[eval_key],\r\n",
    "                                }, \r\n",
    "                                0)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanIoU = MeanIoU(metric_name=\"MeanIoU\")\n",
    "diceCoefficient = DiceCoefficient(metric_name=\"DiCE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, train_loss, valid_loss = train_model(model = model,\r\n",
    "                                            train_generator = train_generator,\r\n",
    "                                            val_generator = val_generator,\r\n",
    "                                            patience = 10,\r\n",
    "                                            n_epochs = args.nEpochs,\r\n",
    "                                            metrics=[diceCoefficient, meanIoU],\r\n",
    "                                            tensorboard=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, generator=test_generator, tensorboard=writer, metrics=[diceCoefficient, meanIoU], args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c431e56cb5cee027ff08fc0ca83c529277af3568c99539597184c60be39cd905"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('torch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "e747c10674bd6bf42aecb79523364f4de5185cd2e4ab2925df4cc61b3557cd22"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}