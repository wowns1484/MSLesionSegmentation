{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import lib.medloaders as medical_loaders\n",
    "import lib.medzoo as medzoo\n",
    "# Lib files\n",
    "import lib.utils as utils\n",
    "from lib.utils.early_stopping import EarlyStopping\n",
    "from lib.utils.general import prepare_input\n",
    "from lib.losses3D import DiceLoss, create_loss, BCEDiceLoss\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from easydict import EasyDict\n",
    "\n",
    "from lib.metric3D.DiceCoefficient import DiceCoefficient\n",
    "from lib.metric3D.MeanIoU import MeanIoU\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = EasyDict({\n",
    "    \"batch_size\" : 1,\n",
    "    \"dataset_name\" : \"miccai2008-mslesions-t2w\",\n",
    "    \"dim\" : (208,224,208),\n",
    "    \"nEpochs\" : 100,\n",
    "    \"classes\" : 1,\n",
    "    \"split\" : (0.8,0.2,0.0),\n",
    "    \"inChannels\" : 1,\n",
    "    \"inModalities\" : 1,\n",
    "    \"fold_id\" : '1',\n",
    "    \"lr\" : 1e-3,\n",
    "    \"cuda\" : True,\n",
    "    \"resume\" : '',\n",
    "    \"model\" : 'NESTEDDENSEUNET3D', # VNET VECT2 UNET3D DENSENET1 DENSENET2 DENSENET3 HYPERDENSENET DENSEUNET3D NESTEDUNET3D NESTEDDENSEUNET3D\n",
    "    \"opt\" : 'adam', # sgd adam rmsprop\n",
    "    \"log_dir\" : 'runs',\n",
    "    \"loadData\" : False,\n",
    "    \"terminal_show_freq\" : 10,\n",
    "    \"channel\" : \"Flair\",\n",
    "})\n",
    "\n",
    "start_time = time.strftime(\"%Y%m%d-%H%M%S\", time.localtime(time.time()))\n",
    "\n",
    "args.result_path = rf'results/{args.dataset_name}-{start_time}'\n",
    "\n",
    "shutil.rmtree(args.result_path, ignore_errors=True)\n",
    "utils.make_dirs(args.result_path)\n",
    "\n",
    "args.save = rf'saved_models/{args.model}_checkpoints/{args.model}-{args.dataset_name}-{start_time}'\n",
    "args.save_checkpoint = os.path.join(args.save,'checkpoint.pt')\n",
    "args.tb_log_dir = rf'runs/{args.model}-{args.dataset_name}-{start_time}'\n",
    "\n",
    "utils.make_dirs(args.tb_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "## FOR REPRODUCIBILITY OF RESULTS\n",
    "seed = 1777777\n",
    "utils.reproducibility(args, seed)\n",
    "utils.make_dirs(args.save)\n",
    "utils.save_arguments(args, args.save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# 기본 `log_dir` 은 \"runs\"이며, 여기서는 더 구체적으로 지정하였습니다\n",
    "writer = SummaryWriter(f'{args.tb_log_dir}/log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchio as tio\n",
    "\n",
    "from torchio.transforms import (\n",
    "    RandomFlip,\n",
    "    RandomAffine,\n",
    "    RandomElasticDeformation, \n",
    "    RandomNoise,\n",
    "    RandomMotion,\n",
    "    RandomBiasField,\n",
    "    RescaleIntensity,\n",
    "    Resample,\n",
    "    ToCanonical,\n",
    "    ZNormalization,\n",
    "    CropOrPad,\n",
    "    HistogramStandardization,\n",
    "    OneOf,\n",
    "    Compose,\n",
    ")\n",
    "\n",
    "### Full-aug ###\n",
    "transform = tio.Compose([\n",
    "    CropOrPad((208,224,208)),\n",
    "    RandomMotion(p=0.2),\n",
    "    RandomBiasField(p=0.3),\n",
    "    RandomNoise(p=0.5),\n",
    "    RandomFlip(axes=(0,)),\n",
    "    RandomAffine(p=0.5),\n",
    "    RandomElasticDeformation(p=0.5),\n",
    "    ZNormalization(),\n",
    "])\n",
    "\n",
    "### selectiv-aug ###\n",
    "# transform = tio.Compose([\n",
    "#     CropOrPad((144,144,144)),\n",
    "#     # RandomMotion(p=0.2),\n",
    "#     # RandomBiasField(p=0.3),\n",
    "#     RandomNoise(p=0.5),\n",
    "#     RandomFlip(axes=(0,)),\n",
    "#     # RandomAffine(p=0.5),\n",
    "#     RandomElasticDeformation(p=0.5),\n",
    "#     ZNormalization(),\n",
    "# ])\n",
    "\n",
    "validation_transform = tio.Compose([\n",
    "    CropOrPad((208,224,208)),\n",
    "    ZNormalization()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.medloaders.miccai_2008_ms_lesions import MICCAI2008MSLESIONS\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = MICCAI2008MSLESIONS(train_mode='train', \n",
    "                                dataset_path=r'D:\\MS-Lesion-Dataset\\MS_Lesion_Challenge',\n",
    "                                classes=args.classes,\n",
    "                                # channel=args.channel,\n",
    "                                crop_dim=args.dim,\n",
    "                                split=args.split,\n",
    "                                transform=transform,\n",
    "                                sample_per_image=10)\n",
    "\n",
    "val_dataset = MICCAI2008MSLESIONS(train_mode='val',\n",
    "                                dataset_path=r'D:\\MS-Lesion-Dataset\\MS_Lesion_Challenge',\n",
    "                                classes=args.classes,\n",
    "                                # channel=args.channel,\n",
    "                                crop_dim=args.dim,\n",
    "                                split=args.split,\n",
    "                                transform=validation_transform,\n",
    "                                sample_per_image=1)\n",
    "\n",
    "params = {\n",
    "        'batch_size': args.batch_size,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 4,\n",
    "        'prefetch_factor' : 2\n",
    "}\n",
    "\n",
    "train_generator = DataLoader(train_dataset, **params)\n",
    "val_generator = DataLoader(val_dataset, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_dataset_path = r'C:\\Users\\VIP444\\Documents\\MS-Lesion-Dataset\\MS_Lesion_Challenge'\n",
    "\n",
    "for idx, (flair, mprage, pdw, t2w, mask) in enumerate(train_dataset.list):\n",
    "    flair = train_dataset.dataset_path + flair.split(pre_dataset_path)[1]\n",
    "    mprage = train_dataset.dataset_path + mprage.split(pre_dataset_path)[1]\n",
    "    pdw = train_dataset.dataset_path + pdw.split(pre_dataset_path)[1]\n",
    "    t2w = train_dataset.dataset_path + t2w.split(pre_dataset_path)[1]\n",
    "    mask = train_dataset.dataset_path + mask.split(pre_dataset_path)[1]\n",
    "    train_dataset.list[idx] = (flair, mprage, pdw, t2w, mask)\n",
    "\n",
    "for idx, (flair, mprage, pdw, t2w, mask) in enumerate(val_dataset.list):\n",
    "    flair = val_dataset.dataset_path + flair.split(pre_dataset_path)[1]\n",
    "    mprage = val_dataset.dataset_path + mprage.split(pre_dataset_path)[1]\n",
    "    pdw = val_dataset.dataset_path + pdw.split(pre_dataset_path)[1]\n",
    "    t2w = val_dataset.dataset_path + t2w.split(pre_dataset_path)[1]\n",
    "    mask = val_dataset.dataset_path + mask.split(pre_dataset_path)[1]\n",
    "    val_dataset.list[idx] = (flair, mprage, pdw, t2w, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.medzoo.ResUnet3D import ResUNet3D\n",
    "from lib.medzoo.DenseUnet3D import DenseUNet3D\n",
    "from lib.medzoo.Nested_DenseUnet3D import NestedDenseUNet3D\n",
    "import torch.optim as optim\n",
    "import torchsummaryX\n",
    "\n",
    "# model = ResUNet3D(in_channels=args.inChannels, n_classes=args.classes, base_n_filter=8)\n",
    "# model = DenseUNet3D(in_channels=args.inChannels, out_channels=args.classes)\n",
    "# model = NestedUNet(in_ch=args.inChannels, out_ch=args.classes)\n",
    "model = NestedDenseUNet3D(in_channels=args.inChannels, out_channels=args.classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=0.0000000001) # weight_decay=0.0000000001\n",
    "\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# torchsummaryX.summary(model, torch.zeros((1,1,208,224,208)).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = BCEDiceLoss(alpha=1, beta=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_generator, val_generator ,patience, n_epochs, metrics, tensorboard=None):\n",
    "\n",
    "    # 모델이 학습되는 동안 trainning loss를 track\n",
    "    train_losses = []\n",
    "    # 모델이 학습되는 동안 validation loss를 track\n",
    "    valid_losses = []\n",
    "    # epoch당 average training loss를 track\n",
    "    avg_train_losses = []\n",
    "    # epoch당 average validation loss를 track\n",
    "    avg_valid_losses = []\n",
    "\n",
    "    train_metrics = [[] for i in range(len(metrics))]\n",
    "    val_metrics = [[] for i in range(len(metrics))]\n",
    "\n",
    "    # early_stopping object의 초기화\n",
    "    early_stopping = EarlyStopping(patience = patience, verbose = True,path=args.save_checkpoint)\n",
    "\n",
    "    n_epochs_length = len(str(n_epochs))\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train() # prep model for training\n",
    "\n",
    "        progressbar = tqdm(train_generator)\n",
    "        for batch, input_tuple in enumerate(progressbar, start=1):\n",
    "        \n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # gpu 연산으로 변경\n",
    "            input, target = prepare_input(input_tuple=input_tuple, args=args)\n",
    "            input.requires_grad = True\n",
    "\n",
    "            # forward pass: 입력된 값을 모델로 전달하여 예측 출력 계산\n",
    "            output = model(input)\n",
    "            output = output.type(torch.FloatTensor)\n",
    "            target = target.type(torch.FloatTensor)\n",
    "\n",
    "            output = output.cpu()\n",
    "            target = target.cpu()\n",
    "\n",
    "            # calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # backward pass: 모델의 파라미터와 관련된 loss의 그래디언트 계산\n",
    "            loss.backward()\n",
    "\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "\n",
    "            # record training loss\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            output[output > 0.5] = 1.0\n",
    "\n",
    "            # record matric\n",
    "            for i,metric in enumerate(metrics):\n",
    "                value = metric(output,target).item()\n",
    "                train_metrics[i].append(value)\n",
    "\n",
    "            # print metric & loss\n",
    "            print_msg = {\n",
    "                'loss' : f'{loss.item():.5f}',\n",
    "            }\n",
    "\n",
    "            for i,metric in enumerate(metrics):\n",
    "                print_msg[metric.metric_name] = f'{train_metrics[i][-1]:.5f}'\n",
    "\n",
    "            progressbar.set_postfix(print_msg)\n",
    "            progressbar.set_description(f'[{epoch:>{n_epochs_length}}/{n_epochs:>{n_epochs_length}}][{batch}/{len(train_generator)}]')\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval() # prep model for evaluation\n",
    "\n",
    "        prediction_images = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i,input_tuple in enumerate(val_generator) :\n",
    "\n",
    "                input, target = prepare_input(input_tuple=input_tuple, args=args)\n",
    "                input.requires_grad = True\n",
    "\n",
    "                # forward pass: 입력된 값을 모델로 전달하여 예측 출력 계산\n",
    "                output = model(input)\n",
    "                output = output.type(torch.FloatTensor)\n",
    "                target = target.type(torch.FloatTensor)\n",
    "\n",
    "                input = input.to(device)\n",
    "                output = output.cpu()\n",
    "                target = target.cpu()\n",
    "\n",
    "                # calculate the loss\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "                # record validation loss\n",
    "                valid_losses.append(loss.item())\n",
    "\n",
    "                # save prediction result\n",
    "                prediction_images.append(\n",
    "                    [input.detach().cpu().numpy(), output.detach().cpu().numpy(), target.detach().cpu().numpy()])\n",
    "\n",
    "                output[output > 0.5] = 1.0\n",
    "\n",
    "                # metric\n",
    "                for i,metric in enumerate(metrics):\n",
    "                    val_metrics[i].append(metric(output, target).item())\n",
    "\n",
    "        columns = 3\n",
    "        rows = len(prediction_images)\n",
    "\n",
    "        fig=plt.figure(figsize=( 3 * columns,3 * rows))\n",
    "\n",
    "        i = 1\n",
    "        for [input,output, target] in prediction_images:\n",
    "            fig.add_subplot(rows, columns, i); i+=1\n",
    "            plt.imshow(input[0,0,:,:,23], cmap='gray')\n",
    "            plt.title(\"original Image\"); plt.axis('off')\n",
    "\n",
    "            fig.add_subplot(rows, columns, i); i+=1\n",
    "            plt.imshow(output[0,0,:,:, 23], cmap='gray')\n",
    "            plt.title(\"Predicited Image\"); plt.axis('off')\n",
    "\n",
    "            fig.add_subplot(rows, columns, i); i+=1\n",
    "            plt.imshow(target[0,0,:,:, 23], cmap='gray')\n",
    "            plt.title(\"Original Mask\"); plt.axis('off')\n",
    "\n",
    "        plt.savefig(f'{args.result_path}/{epoch}.png')\n",
    "        plt.close('all')\n",
    "        plt.clf()\n",
    "\n",
    "        fig = plt.figure(figsize=( 3 * columns,3 * rows))\n",
    "\n",
    "        i = 1\n",
    "        for [input,output, target] in prediction_images:\n",
    "\n",
    "            fig.add_subplot(rows, columns, i); i+=1\n",
    "            plt.imshow(input[0,0,:,:,80], cmap='gray')\n",
    "            plt.title(\"original Image\"); plt.axis('off')\n",
    "\n",
    "            fig.add_subplot(rows, columns, i); i+=1\n",
    "            plt.imshow(output[0,0,:,:, 80], cmap='gray')\n",
    "            plt.title(\"Predicited Image\"); plt.axis('off')\n",
    "\n",
    "            fig.add_subplot(rows, columns, i); i+=1\n",
    "            plt.imshow(target[0,0,:,:, 80], cmap='gray')\n",
    "            plt.title(\"Original Mask\"); plt.axis('off')\n",
    "\n",
    "        plt.savefig(f'{args.result_path}/{epoch}.png')\n",
    "        plt.close('all')\n",
    "        plt.clf()\n",
    "\n",
    "        # print 학습/검증 statistics\n",
    "        # epoch당 평균 loss 계산\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "\n",
    "        # epoch당 평균 metric 계산\n",
    "        train_metric = {}\n",
    "        val_metric = {}\n",
    "\n",
    "        for i, metric in enumerate(metrics):\n",
    "            train_metric[metric.metric_name] = np.average(train_metrics[i])\n",
    "            val_metric[metric.metric_name] = np.average(val_metrics[i])\n",
    "\n",
    "\n",
    "        # epoch 결과 출력\n",
    "        print_msg = (f'[{epoch:>{n_epochs_length}}/{n_epochs:>{n_epochs_length}}] ' +\n",
    "                     f'loss: {train_loss:.5f} ' +\n",
    "                     f'val_loss: {valid_loss:.5f} ')\n",
    "\n",
    "        for key in train_metric.keys():\n",
    "            print_msg += f'{key} : {train_metric[key]:.5f} '\n",
    "\n",
    "        for key in val_metric.keys():\n",
    "            print_msg += f'val_{key} : {val_metric[key]:.5f} '\n",
    "\n",
    "        # tensorboard\n",
    "        if tensorboard:\n",
    "            writer.add_scalars(\"loss\", \n",
    "                                {\n",
    "                                    'train' : train_loss,\n",
    "                                    'val' : valid_loss\n",
    "                                 },\n",
    "                                epoch)\n",
    "\n",
    "            for train_key, val_key in zip(train_metric.keys(), val_metric.keys()):\n",
    "                writer.add_scalars(train_key,\n",
    "                                {\n",
    "                                    'train' : train_metric[train_key],\n",
    "                                     'val' : val_metric[val_key]\n",
    "                                }, \n",
    "                                 epoch)\n",
    "\n",
    "        # clear lists to track next epoch\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        train_metrics = [[] for i in range(len(metrics))]\n",
    "        val_metrics = [[] for i in range(len(metrics))]\n",
    "\n",
    "        # early_stopping는 validation loss가 감소하였는지 확인이 필요하며,\n",
    "        # 만약 감소하였을경우 현제 모델을 checkpoint로 만든다.\n",
    "        early_stopping(valid_loss, model)\n",
    "\n",
    "        # if early_stopping.early_stop:\n",
    "        #     print(\"Early stopping\")\n",
    "        #     break\n",
    "\n",
    "   # best model이 저장되어있는 last checkpoint를 로드한다.\n",
    "    model.load_state_dict(torch.load(args.save_checkpoint))\n",
    "    writer.close()\n",
    "\n",
    "    return  model, avg_train_losses, avg_valid_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanIoU = MeanIoU(metric_name=\"MeanIoU\")\n",
    "diceCoefficient = DiceCoefficient(metric_name=\"DiCE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, train_loss, valid_loss = train_model(\n",
    "    model = model,\n",
    "    train_generator = train_generator,\n",
    "    val_generator = val_generator,\n",
    "    patience = 5,\n",
    "    n_epochs = args.nEpochs,\n",
    "    metrics=[diceCoefficient, meanIoU],\n",
    "    tensorboard=writer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), r'C:\\Users\\VIP444\\Documents\\Github\\MS-Lesions-Pytorch\\saved_models\\NESTEDDENSEUNET3D_checkpoints\\NESTEDDENSEUNET3D-miccai2008-mslesions-t2w-20220921-111810' + '\\model_state_dict.pt')\n",
    "torch.save(model, r'C:\\Users\\VIP444\\Documents\\Github\\MS-Lesions-Pytorch\\saved_models\\NESTEDDENSEUNET3D_checkpoints\\NESTEDDENSEUNET3D-miccai2008-mslesions-t2w-20220921-111810' + '\\model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = torch.load(r'C:\\Users\\VIP444\\Documents\\Github\\MS-Lesions-Pytorch\\saved_models\\NESTEDDENSEUNET3D_checkpoints\\NESTEDDENSEUNET3D-miccai2008-mslesions-flair-20220919-114041' + '\\model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchio as tio\n",
    "from lib.medzoo.Unet3D import Unet3D\n",
    "from lib.medzoo.ResUnet3D import ResUNet3D\n",
    "from lib.medzoo.Nested_DenseUnet3D import NestedDenseUNet3D\n",
    "\n",
    "path = r'C:\\Users\\VIP444\\Documents\\Github\\MS-Lesions-Pytorch\\saved_models\\NESTEDDENSEUNET3D_checkpoints\\NESTEDDENSEUNET3D-miccai2008-mslesions-flair-20220919-114041' + '\\model_state_dict.pt'\n",
    "check_point = r'C:\\Users\\VIP444\\Documents\\Github\\MS-Lesions-Pytorch\\saved_models\\NESTEDDENSEUNET3D_checkpoints\\NESTEDDENSEUNET3D-miccai2008-mslesions-flair-20220419-180313\\checkpoint.pt'\n",
    "test_model = NestedDenseUNet3D(in_channels=args.inChannels, out_channels=args.classes)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    test_model = test_model.cuda()\n",
    "\n",
    "test_model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.medloaders.miccai_2008_ms_lesions import MICCAI2008MSLESIONS\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_transform = tio.Compose([\n",
    "    tio.CropOrPad((208,224,208)),\n",
    "    tio.ZNormalization()\n",
    "])\n",
    "\n",
    "test_dataset = MICCAI2008MSLESIONS(\n",
    "                                train_mode='test',\n",
    "                                dataset_path=r'D:\\MS-Lesion-Dataset\\MS_Lesion_Challenge',\n",
    "                                classes=args.classes, \n",
    "                                crop_dim=args.dim,\n",
    "                                transform=test_transform\n",
    "                                )\n",
    "\n",
    "params = {\n",
    "        'batch_size': args.batch_size,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 4\n",
    "        }\n",
    "\n",
    "test_generator = DataLoader(test_dataset, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_dataset_path = r'C:\\Users\\VIP444\\Documents\\MS-Lesions-Segmentation\\MS-Lesions-Pytorch\\datasets\\MICCAI_2008_MS_Lesions'\n",
    "\n",
    "for idx, (flair, mprage, pdw, t2w) in enumerate(test_dataset.list):\n",
    "    flair = test_dataset.dataset_path + flair.split(pre_dataset_path)[1]\n",
    "    mprage = test_dataset.dataset_path + mprage.split(pre_dataset_path)[1]\n",
    "    pdw = test_dataset.dataset_path + pdw.split(pre_dataset_path)[1]\n",
    "    t2w = test_dataset.dataset_path + t2w.split(pre_dataset_path)[1]\n",
    "    test_dataset.list[idx] = (flair, mprage, pdw, t2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "from lib import utils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "output_name = 'KIT'\n",
    "\n",
    "test_result =r'C:\\Users\\VIP444\\Desktop\\test_result'\n",
    "utils.make_dirs(test_result)\n",
    "\n",
    "test_model.eval() # prep model for evaluation\n",
    "prediction_images = []\n",
    "\n",
    "transform = tio.CropOrPad(args.dim)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i,input_tuple in enumerate(test_generator):\n",
    "        # forward pass: 입력된 값을 모델로 전달하여 예측 출력 계산\n",
    "        # [1,1,208, 224, 208]\n",
    "        # [batch, channel, width, height, depth]\n",
    "        input = input_tuple[0].cuda()\n",
    "\n",
    "        target = input_tuple[1].cuda()\n",
    "        # path = input_tuple[1][0]\n",
    "\n",
    "        input.requires_grad = True\n",
    "        output = test_model(input)\n",
    "\n",
    "        input = input.cpu()\n",
    "        output = output.cpu()\n",
    "        target = target.cpu()\n",
    "\n",
    "        # CLAMP 01\n",
    "        output[output >= 0.5] = 1.0\n",
    "        output[output < 0.5] = 0.0\n",
    "\n",
    "        # torch \n",
    "        output = torch.squeeze(output,0)\n",
    "        # crop [1,208,224,208] -> [1,181,217,181]\n",
    "        # output = transform(output)\n",
    "        # squeeze [181,217, 181]\n",
    "        output = torch.squeeze(output,0)\n",
    "        target = torch.squeeze(target,0)\n",
    "        target = torch.squeeze(target,0)\n",
    "\n",
    "        # save prediction result\n",
    "        prediction_images.append(\n",
    "            [input.detach().numpy(),output.detach().numpy(), target.detach().numpy()])\n",
    "\n",
    "        # path_basename = '_'.join(os.path.basename(path).split('_')[:2])\n",
    "        # affine = nib.load(path).affine\n",
    "\n",
    "        # nii_image = nib.Nifti1Image(output.detach().numpy(), test_dataset.affine)\n",
    "        # nib.save(nii_image, f'{test_result}/{path_basename}_KIT_{i}.nii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = 2\n",
    "rows = len(prediction_images)\n",
    "\n",
    "fig=plt.figure(figsize=( 3 * columns,3 * rows))\n",
    "\n",
    "i = 1\n",
    "for [_input,_output] in prediction_images:\n",
    "    fig.add_subplot(rows, columns, i); i+=1\n",
    "    plt.imshow(_input[0,0,:,:,_input.shape[-1] // 2], cmap='gray')\n",
    "    plt.title(\"original Image\"); plt.axis('off')\n",
    "\n",
    "    fig.add_subplot(rows, columns, i); i+=1\n",
    "    plt.imshow(_output[:,:,_output.shape[-1] // 2], cmap='gray')\n",
    "    plt.title(\"Predicited Image\"); plt.axis('off')\n",
    "\n",
    "    plt.savefig(f'{test_result}/test_KIT_{i}.png')\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "af640ef5e237560663450db179292dda4e36d8af96316b6d53ad8d34500a15be"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "metadata": {
   "interpreter": {
    "hash": "e747c10674bd6bf42aecb79523364f4de5185cd2e4ab2925df4cc61b3557cd22"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
